{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import util.image_import as ii\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import mobilenet\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from util.plots import plot_history\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick to configure gpu memory dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = K.tf.ConfigProto() # Config to dynamically add memory\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))# set this TensorFlow session as the default session for Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtracting mean and normalizing to save pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the images\n",
    "train_pot = glob(os.path.join(\"../dataset2/res_still/train/potato\", \"*.jpg\"))\n",
    "train_cat = glob(os.path.join(\"../dataset2/res_still/train/catfood\", \"*.jpg\"))\n",
    "train_tab = glob(os.path.join(\"../dataset2/res_still/train/table\", \"*.jpg\"))\n",
    "val_pot   = glob(os.path.join(\"../dataset2/res_still/valid/potato\", \"*.jpg\"))\n",
    "val_cat   = glob(os.path.join(\"../dataset2/res_still/valid/catfood\", \"*.jpg\"))\n",
    "val_tab   = glob(os.path.join(\"../dataset2/res_still/valid/table\", \"*.jpg\"))\n",
    "test_pot  = glob(os.path.join(\"../dataset2/res_still/test/potato\", \"*.jpg\"))\n",
    "test_cat  = glob(os.path.join(\"../dataset2/res_still/test/catfood\", \"*.jpg\"))\n",
    "test_tab  = glob(os.path.join(\"../dataset2/res_still/test/table\", \"*.jpg\"))\n",
    "\n",
    "X_train, y_train = ii.images_to_numpy(train_pot, train_cat, train_tab)\n",
    "X_val, y_val = ii.images_to_numpy(val_pot, val_cat, val_tab)\n",
    "X_test, y_test = ii.images_to_numpy(test_pot, test_cat, test_tab)\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 3\n",
    "\n",
    "# Making datagen for training with normilization, center, and dataugmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        #featurewise_std_normalization=True\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=True,\n",
    "        brightness_range=(0.65, 1.35)\n",
    ")\n",
    "\n",
    "# Making datagen for validation and test with normilization\n",
    "valid_datagen = ImageDataGenerator(\n",
    "        featurewise_center=True, \n",
    "        #featurewise_std_normalization=True\n",
    "        rescale=1./255\n",
    ")\n",
    "\n",
    "\n",
    "X_train = X_train.astype(float)\n",
    "X_val = X_val.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_val = to_categorical(y_val, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Required for featurewise normalization\n",
    "train_datagen.fit(X_train/255.)\n",
    "valid_datagen.fit(X_train/255.)\n",
    "\n",
    "train_generator = train_datagen.flow(X_train,\n",
    "                                     y_train,\n",
    "                                     batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_generator = valid_datagen.flow(X_val,\n",
    "                                     y_val,\n",
    "                                     batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_generator = valid_datagen.flow(X_test,\n",
    "                                     y_test,\n",
    "                                     batch_size=80, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized model single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_optimized = Sequential()\n",
    "model_optimized.add(Flatten(input_shape=(224,224,3)))\n",
    "model_optimized.add(Dense(256))\n",
    "model_optimized.add(Activation('relu'))\n",
    "model_optimized.add(Dense(3))\n",
    "model_optimized.add(Activation('softmax'))\n",
    "model_optimized.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=1e-5), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_optimized.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_optimized = model_optimized.fit_generator(\n",
    "                    train_generator,\n",
    "                    steps_per_epoch=573 // batch_size,\n",
    "                    epochs=100,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=160 // batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_optimized, save=True, path='nets/feedforward/', name='plot_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(test_generator)\n",
    "\n",
    "#ynew = model.predict(imgs)\n",
    "#results[(lr, lrd, rs, hl)] = (y_train_acc, y_pred_acc)\n",
    "\n",
    "history_optimized.evaluate(imgs,labels, batch_size=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(validation_generator)\n",
    "history_optimized.predict(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only work with tensorflow=1.8 and keras=2.1.6 because of the memory allocation\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rates = [1e-4, 1e-5];\n",
    "hidden_size = [128, 256, 512];\n",
    "history = []\n",
    "result = None\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for hs in hidden_size:\n",
    "        # GPU memory allocation\n",
    "        cfg = K.tf.ConfigProto() # Create setup file for keras tensorflow\n",
    "        cfg.gpu_options.allow_growth = True # Setting setup file with dynamically adding gpu memory\n",
    "        K.set_session(K.tf.Session(config=cfg)) # Setting session to use setup file\n",
    "        sess = tf.Session() \n",
    "        K.set_session(sess)# set this TensorFlow session as the default session for Keras.\n",
    "        \n",
    "        # Defining the model:\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(224,224,3)))\n",
    "        model.add(Dense(hs))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(3))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=lr),\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "        # Running the model:\n",
    "        temp = (model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=573 // batch_size,\n",
    "            epochs=100,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=160 // batch_size))\n",
    "        \n",
    "        # Saving the results\n",
    "        history.append(temp)\n",
    "        train_acc = temp.history['acc'].pop()\n",
    "        val_acc = temp.history['val_acc'].pop()\n",
    "        result = (lr, hs, train_acc, val_acc)\n",
    "        results.append(result)\n",
    "        K.clear_session() # Clearing the session to clear out gpu memory\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing all results and plotting only the best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valacc_indices = []\n",
    "highest_valacc = 0\n",
    "for index, r in enumerate(results):\n",
    "    print('lr %e hs %i train accuracy: %f val accuracy: %f' % r) \n",
    "    if highest_valacc < r[3]:\n",
    "        best_valacc_indices = [] # Empty the list because of better value\n",
    "        best_valacc_indices.append(index)\n",
    "        highest_valacc = r[3]\n",
    "    elif highest_valacc == r[3]:\n",
    "        best_valacc_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for indices in best_valacc_indices:\n",
    "    print(\"______________________________________________________________________\")\n",
    "    print('lr %e hs %i train accuracy: %f val accuracy: %f' % results[indices])         \n",
    "    plot_history(history[indices], mean_N=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on graph and results above of the training and validation accuracy the following hyperparameters is found to be the optimal:** \n",
    "```\n",
    "Learning rate       = 1e-5\n",
    "Hidden size         = 256\n",
    "```\n",
    "**Which gives the following results for 100 epoch for 1 training run:**\n",
    "```\n",
    "Train accuracy      = 0.987500\n",
    "Validation accuracy = 0.983364\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and standard deviation for optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17/17 [==============================] - 5s 308ms/step - loss: 1.1183 - acc: 0.4017 - val_loss: 1.0259 - val_acc: 0.3812\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 5s 275ms/step - loss: 0.9543 - acc: 0.5551 - val_loss: 0.9265 - val_acc: 0.7812\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 5s 280ms/step - loss: 0.8905 - acc: 0.5945 - val_loss: 0.8609 - val_acc: 0.6250\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 5s 286ms/step - loss: 0.8178 - acc: 0.6471 - val_loss: 0.7654 - val_acc: 0.8625\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 5s 279ms/step - loss: 0.7525 - acc: 0.7302 - val_loss: 0.8268 - val_acc: 0.6438\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 5s 284ms/step - loss: 0.7209 - acc: 0.7518 - val_loss: 0.7237 - val_acc: 0.8250\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 5s 281ms/step - loss: 0.6669 - acc: 0.7694 - val_loss: 0.7261 - val_acc: 0.8063\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 5s 289ms/step - loss: 0.6357 - acc: 0.7792 - val_loss: 0.6122 - val_acc: 0.7937\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 5s 287ms/step - loss: 0.5855 - acc: 0.8134 - val_loss: 0.6007 - val_acc: 0.8375\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 5s 282ms/step - loss: 0.5792 - acc: 0.8181 - val_loss: 0.6289 - val_acc: 0.7625\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 5s 282ms/step - loss: 0.6679 - acc: 0.7408 - val_loss: 0.5567 - val_acc: 0.8500\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 5s 275ms/step - loss: 0.5411 - acc: 0.8340 - val_loss: 0.6417 - val_acc: 0.8063\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 5s 280ms/step - loss: 0.5613 - acc: 0.8057 - val_loss: 0.5771 - val_acc: 0.8313\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 5s 298ms/step - loss: 0.5117 - acc: 0.8448 - val_loss: 0.4609 - val_acc: 0.9000\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 6s 347ms/step - loss: 0.4384 - acc: 0.8695 - val_loss: 0.4368 - val_acc: 0.9250\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 5s 322ms/step - loss: 0.4253 - acc: 0.8930 - val_loss: 0.5310 - val_acc: 0.8250\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 6s 328ms/step - loss: 0.4165 - acc: 0.8739 - val_loss: 0.4385 - val_acc: 0.8438\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 6s 345ms/step - loss: 0.4178 - acc: 0.8798 - val_loss: 0.4571 - val_acc: 0.8625\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 6s 335ms/step - loss: 0.3936 - acc: 0.9040 - val_loss: 0.4242 - val_acc: 0.8313\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 6s 337ms/step - loss: 0.3976 - acc: 0.8871 - val_loss: 0.3748 - val_acc: 0.9250\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 6s 333ms/step - loss: 0.3538 - acc: 0.9042 - val_loss: 0.3791 - val_acc: 0.9125\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 6s 342ms/step - loss: 0.3444 - acc: 0.9108 - val_loss: 0.3184 - val_acc: 0.9375\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 6s 340ms/step - loss: 0.3289 - acc: 0.9265 - val_loss: 0.3101 - val_acc: 0.9563\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - 6s 334ms/step - loss: 0.2807 - acc: 0.9410 - val_loss: 0.3385 - val_acc: 0.8500\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - 6s 335ms/step - loss: 0.3552 - acc: 0.8939 - val_loss: 0.3320 - val_acc: 0.9250\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - 6s 332ms/step - loss: 0.3078 - acc: 0.9208 - val_loss: 0.3112 - val_acc: 0.9313\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - 6s 333ms/step - loss: 0.3271 - acc: 0.9090 - val_loss: 0.3470 - val_acc: 0.9125\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - 6s 349ms/step - loss: 0.2976 - acc: 0.9173 - val_loss: 0.2911 - val_acc: 0.9250\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - 6s 343ms/step - loss: 0.2733 - acc: 0.9410 - val_loss: 0.3057 - val_acc: 0.8562\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - 6s 346ms/step - loss: 0.2665 - acc: 0.9406 - val_loss: 0.2851 - val_acc: 0.9187\n",
      "Epoch 1/30\n",
      "17/17 [==============================] - 6s 349ms/step - loss: 0.2635 - acc: 0.9540 - val_loss: 0.2789 - val_acc: 0.9375\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 6s 352ms/step - loss: 0.2698 - acc: 0.9355 - val_loss: 0.2487 - val_acc: 0.9563\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 6s 340ms/step - loss: 0.2579 - acc: 0.9412 - val_loss: 0.2555 - val_acc: 0.9625\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 6s 347ms/step - loss: 0.2606 - acc: 0.9358 - val_loss: 0.2445 - val_acc: 0.9688\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 6s 345ms/step - loss: 0.2354 - acc: 0.9448 - val_loss: 0.2707 - val_acc: 0.9313\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 6s 333ms/step - loss: 0.2238 - acc: 0.9614 - val_loss: 0.2131 - val_acc: 0.9437\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 6s 338ms/step - loss: 0.2234 - acc: 0.9511 - val_loss: 0.2375 - val_acc: 0.9250\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 6s 344ms/step - loss: 0.2252 - acc: 0.9596 - val_loss: 0.2140 - val_acc: 0.9563\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 6s 348ms/step - loss: 0.2140 - acc: 0.9559 - val_loss: 0.1897 - val_acc: 0.9688\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 6s 342ms/step - loss: 0.1970 - acc: 0.9577 - val_loss: 0.2096 - val_acc: 0.9437\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 6s 359ms/step - loss: 0.2235 - acc: 0.9537 - val_loss: 0.1994 - val_acc: 0.9750\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 6s 342ms/step - loss: 0.2121 - acc: 0.9357 - val_loss: 0.1890 - val_acc: 0.9500\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 6s 347ms/step - loss: 0.2289 - acc: 0.9412 - val_loss: 0.1742 - val_acc: 0.9688\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 6s 344ms/step - loss: 0.2013 - acc: 0.9557 - val_loss: 0.1974 - val_acc: 0.9625\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 6s 339ms/step - loss: 0.1928 - acc: 0.9662 - val_loss: 0.1763 - val_acc: 0.9812\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 6s 349ms/step - loss: 0.2015 - acc: 0.9577 - val_loss: 0.1622 - val_acc: 0.9688\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 6s 337ms/step - loss: 0.1670 - acc: 0.9643 - val_loss: 0.1743 - val_acc: 0.9625\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 6s 331ms/step - loss: 0.1812 - acc: 0.9665 - val_loss: 0.1685 - val_acc: 0.9563\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 6s 352ms/step - loss: 0.1590 - acc: 0.9739 - val_loss: 0.1538 - val_acc: 0.9812\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 6s 332ms/step - loss: 0.1615 - acc: 0.9687 - val_loss: 0.1605 - val_acc: 0.9688\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 6s 352ms/step - loss: 0.1537 - acc: 0.9761 - val_loss: 0.1688 - val_acc: 0.9812\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 6s 344ms/step - loss: 0.1675 - acc: 0.9614 - val_loss: 0.1695 - val_acc: 0.9688\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 6s 349ms/step - loss: 0.2101 - acc: 0.9375 - val_loss: 0.1488 - val_acc: 0.9750\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - 6s 341ms/step - loss: 0.1556 - acc: 0.9761 - val_loss: 0.1774 - val_acc: 0.9563\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - 6s 345ms/step - loss: 0.1387 - acc: 0.9704 - val_loss: 0.1748 - val_acc: 0.9812\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - 6s 353ms/step - loss: 0.1384 - acc: 0.9796 - val_loss: 0.1372 - val_acc: 0.9688\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - 6s 367ms/step - loss: 0.1462 - acc: 0.9761 - val_loss: 0.1402 - val_acc: 0.9688\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - 6s 349ms/step - loss: 0.1517 - acc: 0.9687 - val_loss: 0.1293 - val_acc: 0.9625\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - 6s 350ms/step - loss: 0.1328 - acc: 0.9835 - val_loss: 0.1403 - val_acc: 0.9625\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - 6s 353ms/step - loss: 0.1260 - acc: 0.9796 - val_loss: 0.1359 - val_acc: 0.9688\n"
     ]
    }
   ],
   "source": [
    "history_mean_std = []\n",
    "iterations = 30\n",
    "\n",
    "# Defining the model:\n",
    "model_mean_std = Sequential()\n",
    "model_mean_std.add(Flatten(input_shape=(224,224,3)))\n",
    "model_mean_std.add(Dense(256))\n",
    "model_mean_std.add(Activation('relu'))\n",
    "model_mean_std.add(Dense(3))\n",
    "model_mean_std.add(Activation('softmax'))\n",
    "model_mean_std.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=Adam(lr=1e-5),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "for it in range(iterations):\n",
    "        # Training the model:\n",
    "        temp = (model_mean_std.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=573 // batch_size,\n",
    "            epochs=30,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=160 // batch_size))\n",
    "        \n",
    "        # Saving the results\n",
    "        history_mean_std.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Mean------------------\n",
      "Train Loss:     0.19606438312892993\n",
      "Train Accuracy: 0.9602587796513562\n",
      "Valid Loss:     0.2105408251285553\n",
      "Valid Accuracy: 0.94375\n",
      "---------Standard Deviation-----------\n",
      "Train Loss:     0.07001356870734758\n",
      "Train Accuracy: 0.019408502166681052\n",
      "Valid Loss:     0.07460179030895234\n",
      "Valid Accuracy: 0.025000000000000022\n"
     ]
    }
   ],
   "source": [
    "temp = None\n",
    "temp_his = []\n",
    "model_mean = None\n",
    "model_std = None\n",
    "\n",
    "for h in history_mean_std:\n",
    "    temp = (h.history['loss'], h.history['acc'], h.history['val_loss'], h.history['val_acc'])\n",
    "    temp_his.append(temp)\n",
    "\n",
    "temp_his = np.asarray(temp_his) # Making it to numpy array\n",
    "model_mean = np.mean(temp_his, axis=0) # Calculating mean for all points\n",
    "model_std = np.std(temp_his, axis=0) # Standard deviation for all points\n",
    "\n",
    "# Brug plt.plot for at vise plots\n",
    "\n",
    "# Printing the last value of loss, acc, vall_loss and vall_acc\n",
    "string_val = [\"Train Loss:     \", \"Train Accuracy: \", \"Valid Loss:     \", \"Valid Accuracy: \"]\n",
    "\n",
    "print(\"----------------Mean------------------\")\n",
    "for index, i in enumerate(model_mean):\n",
    "    print(string_val[index] + str(i[-1]))\n",
    "    \n",
    "print(\"---------Standard Deviation-----------\")\n",
    "for index, i in enumerate(model_std):\n",
    "    print(string_val[index] + str(i[-1]))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
