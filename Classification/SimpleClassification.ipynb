{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Purpose of this notebook</h1>\n",
    "The purpose of this notebook is to see the effects of data augmentation.\n",
    "\n",
    "Data augmentation is when the data is augmented to generate multiple versions of the same image.\n",
    "\n",
    "When only a small dataset of images is available it might show to be beneficial to use data augmentation.\n",
    "Since the neural network might be overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import mobilenet\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Resources used to train, validate and test.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../Retraining/resources/train'\n",
    "valid_path = '../Retraining/resources/valid'\n",
    "test_path = '../Retraining/resources/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ImageDataGenerators: Without data augmentation</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 299 images belonging to 3 classes.\n",
      "Found 82 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_datagen_wo = ImageDataGenerator(\n",
    "        rescale=1./255\n",
    ")\n",
    "\n",
    "valid_datagen_wo = ImageDataGenerator(\n",
    "        rescale=1./255\n",
    ")\n",
    "\n",
    "# Train data generator\n",
    "train_generator_wo = train_datagen_wo.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        classes=['potato', 'catfood', 'table'])\n",
    "\n",
    "# Validation data generator\n",
    "validation_generator_wo = valid_datagen_wo.flow_from_directory(\n",
    "        valid_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        classes=['potato', 'catfood', 'table'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ImageDataGenerators: With data augmentation</h1>\n",
    "<ul>\n",
    "    <li> <b>Rescale</b> rescales image data from range [0 255] to range [0 1] </li>\n",
    "    <li><b>Shear_range</b> a range with how much the data is augmented with respect to shear</li>\n",
    "    <li><b>Zoom_range</b> range for a random zoom. [1-zoom_range 1+zoom_range]</li>\n",
    "    <li><b>Horizontal_flip:</b> boolean if it should be possible to make a random horizontal flip</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "Should be tested:\n",
    "<ul>\n",
    "    <li><b>Rotation_range</b></li>\n",
    "    <li><b>brightness_range</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 299 images belonging to 3 classes.\n",
      "Found 82 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "        rescale=1./255\n",
    ")\n",
    "\n",
    "# Train data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        classes=['potato', 'catfood', 'table'])\n",
    "\n",
    "# Validation data generator\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        valid_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        classes=['potato', 'catfood', 'table'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 222, 222, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 109, 109, 64)      18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 109, 109, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 186624)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                11944000  \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 11,963,587\n",
      "Trainable params: 11,963,587\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Utility functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    #print(history.history.keys())\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Testing performance with data augmentation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 13s 740ms/step - loss: 1.7101 - acc: 0.5312 - val_loss: 0.6239 - val_acc: 0.6220\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 3s 158ms/step - loss: 0.5864 - acc: 0.6899 - val_loss: 0.3759 - val_acc: 0.8537\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 3s 161ms/step - loss: 0.6614 - acc: 0.7205 - val_loss: 0.3591 - val_acc: 0.8902\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.4818 - acc: 0.7880 - val_loss: 0.4025 - val_acc: 0.8415\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 3s 191ms/step - loss: 0.4425 - acc: 0.8019 - val_loss: 0.3810 - val_acc: 0.8415\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 0.3432 - acc: 0.8093 - val_loss: 0.5983 - val_acc: 0.7683\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.4004 - acc: 0.8133 - val_loss: 0.3452 - val_acc: 0.8659\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.4413 - acc: 0.8456 - val_loss: 0.3638 - val_acc: 0.8659\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 3s 145ms/step - loss: 0.3175 - acc: 0.8333 - val_loss: 0.4501 - val_acc: 0.8780\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 0.3343 - acc: 0.8668 - val_loss: 0.3997 - val_acc: 0.8780\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.3396 - acc: 0.8368 - val_loss: 0.4706 - val_acc: 0.8659\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.2337 - acc: 0.9151 - val_loss: 0.6251 - val_acc: 0.9024\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.5907 - acc: 0.8401 - val_loss: 0.7091 - val_acc: 0.8780\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.4416 - acc: 0.8476 - val_loss: 0.5948 - val_acc: 0.9024\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 3s 148ms/step - loss: 0.2817 - acc: 0.8873 - val_loss: 0.8267 - val_acc: 0.8049\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 3s 148ms/step - loss: 0.2451 - acc: 0.8773 - val_loss: 0.6655 - val_acc: 0.9146\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 3s 145ms/step - loss: 5.5747 - acc: 0.6386 - val_loss: 1.9492 - val_acc: 0.6829\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 0.4926 - acc: 0.8476 - val_loss: 0.6768 - val_acc: 0.9024\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 3s 143ms/step - loss: 0.2292 - acc: 0.8773 - val_loss: 0.7540 - val_acc: 0.9146\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.2064 - acc: 0.9186 - val_loss: 1.1358 - val_acc: 0.8293\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 3s 148ms/step - loss: 0.3411 - acc: 0.8674 - val_loss: 0.8070 - val_acc: 0.9146\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.2684 - acc: 0.8958 - val_loss: 0.8628 - val_acc: 0.9146\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.2310 - acc: 0.8996 - val_loss: 0.9582 - val_acc: 0.8902\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 3s 145ms/step - loss: 0.2809 - acc: 0.8962 - val_loss: 0.8747 - val_acc: 0.9024\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 0.1540 - acc: 0.9305 - val_loss: 1.0271 - val_acc: 0.9146\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.1686 - acc: 0.9151 - val_loss: 0.8984 - val_acc: 0.9268\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 0.3290 - acc: 0.8924 - val_loss: 0.7902 - val_acc: 0.9268\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.2452 - acc: 0.9135 - val_loss: 0.8077 - val_acc: 0.8902\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 3s 148ms/step - loss: 0.1411 - acc: 0.9514 - val_loss: 0.9173 - val_acc: 0.9024\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 3s 145ms/step - loss: 0.6074 - acc: 0.8633 - val_loss: 0.8425 - val_acc: 0.9024\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 3s 151ms/step - loss: 0.1130 - acc: 0.9444 - val_loss: 0.9467 - val_acc: 0.9146\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 2s 136ms/step - loss: 0.3071 - acc: 0.8822 - val_loss: 0.8296 - val_acc: 0.8780\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 0.8127 - acc: 0.8550 - val_loss: 0.8646 - val_acc: 0.9268\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 3s 145ms/step - loss: 0.1953 - acc: 0.9097 - val_loss: 0.8238 - val_acc: 0.9268\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 3s 144ms/step - loss: 0.2106 - acc: 0.9513 - val_loss: 0.8623 - val_acc: 0.9146\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 3s 149ms/step - loss: 0.3134 - acc: 0.9047 - val_loss: 0.8408 - val_acc: 0.9268\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 3s 163ms/step - loss: 0.1934 - acc: 0.9186 - val_loss: 0.8481 - val_acc: 0.9024\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 3s 150ms/step - loss: 0.1585 - acc: 0.9186 - val_loss: 1.1083 - val_acc: 0.9146\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 3s 150ms/step - loss: 0.2969 - acc: 0.9062 - val_loss: 0.8824 - val_acc: 0.9268\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 3s 160ms/step - loss: 0.1608 - acc: 0.9444 - val_loss: 0.9345 - val_acc: 0.8902\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 3s 149ms/step - loss: 0.2815 - acc: 0.9132 - val_loss: 0.8130 - val_acc: 0.9268\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.1268 - acc: 0.9498 - val_loss: 0.9809 - val_acc: 0.9146\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 3s 154ms/step - loss: 0.1685 - acc: 0.9270 - val_loss: 0.8754 - val_acc: 0.9268\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.1566 - acc: 0.9340 - val_loss: 0.8211 - val_acc: 0.9146\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 3s 161ms/step - loss: 0.1723 - acc: 0.9549 - val_loss: 0.9608 - val_acc: 0.9024\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 3s 154ms/step - loss: 0.2107 - acc: 0.9364 - val_loss: 0.8845 - val_acc: 0.9024\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 3s 156ms/step - loss: 0.2228 - acc: 0.9096 - val_loss: 0.9887 - val_acc: 0.9146\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 3s 151ms/step - loss: 0.9166 - acc: 0.8715 - val_loss: 0.9526 - val_acc: 0.8902\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.2191 - acc: 0.9032 - val_loss: 0.9116 - val_acc: 0.9024\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 3s 150ms/step - loss: 0.1335 - acc: 0.9394 - val_loss: 1.0303 - val_acc: 0.9024\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=299 // batch_size,\n",
    "            epochs=50,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=82 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255\n",
    ")\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "test_generator = valid_datagen.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        classes=['potato', 'catfood', 'table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "16/16 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2578544020652771, 0.875]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(test_generator)\n",
    "\n",
    "#ynew = model.predict(imgs)\n",
    "\n",
    "model.evaluate(imgs,labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Retraining/tfpoet/tf_files/test/gulerod.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e337edce352b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgul_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../Retraining/tfpoet/tf_files/test/gulerod.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgul_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/sdb/anaconda/envs/BScPRO/lib/python3.5/site-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    493\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    494\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 495\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/sdb/anaconda/envs/BScPRO/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Retraining/tfpoet/tf_files/test/gulerod.jpg'"
     ]
    }
   ],
   "source": [
    "gul_path = '../Retraining/tfpoet/tf_files/test/gulerod.jpg'\n",
    "img = image.load_img(gul_path, target_size=(224, 224))\n",
    "img = image.img_to_array(img)\n",
    "img = img\n",
    "img = np.expand_dims(img, axis=0)\n",
    "out = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Testing performance without data augmentation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "            train_generator_wo,\n",
    "            steps_per_epoch=299 // batch_size,\n",
    "            epochs=50,\n",
    "            validation_data=validation_generator_wo,\n",
    "            validation_steps=82 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(test_generator)\n",
    "\n",
    "#ynew = model.predict(imgs)\n",
    "\n",
    "model.evaluate(imgs,labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Testing with original image size: 1280x720</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../potato_and_catfood/train'\n",
    "valid_path = '../potato_and_catfood/valid'\n",
    "test_path = '../potato_and_catfood/test'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
